{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63531685-0048-4219-9569-bf0da2c17a24",
   "metadata": {},
   "source": [
    "# Find indices for Long-tail Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914bb1ac-941d-4238-92e2-7964a880767f",
   "metadata": {},
   "source": [
    "First, we load the PyTorch CIFAR10 dataset that the indices will be based off of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50511c0e-7139-48e6-bc43-8727d7ba5ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "cifar10_dataset = CIFAR10(root=\"../data\", train=True, download=True, transform=transform)\n",
    "cifar10_images = [(255 * image.numpy()).astype(np.uint8) for image, _ in cifar10_dataset]\n",
    "# np.in1d trick - https://stackoverflow.com/a/16216866\n",
    "cifar10_images_bytes = np.array([cifar10_image.tobytes() for cifar10_image in cifar10_images])\n",
    "cifar10_labels = [label for _, label in cifar10_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d923916-9cb1-41e1-b463-f636cafd372d",
   "metadata": {},
   "source": [
    "We do the same for CIFAR100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d3252d-8530-4cd1-b373-0d89f25d44e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "cifar100_dataset = CIFAR100(root=\"../data\", train=True, download=True, transform=transform)\n",
    "cifar100_images = [(255 * image.numpy()).astype(np.uint8) for image, _ in cifar100_dataset]\n",
    "# np.in1d trick - https://stackoverflow.com/a/16216866\n",
    "cifar100_images_bytes = np.array([cifar100_images.tobytes() for cifar100_images in cifar100_images])\n",
    "cifar100_labels = [label for _, label in cifar100_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89ba99-fd7e-4e00-83c4-4dcf7271de43",
   "metadata": {},
   "source": [
    "Then, we download the tfrecords from Cui et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680959d4-1c8e-4582-a332-549c5f9942a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import gdown\n",
    "\n",
    "if not os.path.exists(\"tfrecords/data/\"):\n",
    "    # Download from Google Drive\n",
    "    # https://github.com/richardaecn/class-balanced-loss/blob/master/README.md#datasets\n",
    "    url = \"https://drive.google.com/uc?id=1NY3lWYRfsTWfsjFPxJUlPumy-WFeD7zK\"\n",
    "    output = \"tfrecords.zip\"\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "    # Unzip to `tfrecords/data/`\n",
    "    with zipfile.ZipFile(\"tfrecords.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"tfrecords\")\n",
    "\n",
    "    # Cleanup\n",
    "    os.remove(\"tfrecords.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87a026-d138-44ae-ad56-705e49551a46",
   "metadata": {},
   "source": [
    "We can now compare images one-by-one to find and save indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676c0e7d-ddba-450e-a486-45562dec49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_to_indices = {\n",
    "    \"tfrecords/data/cifar-10-data-im-0.1/train.tfrecords\": \"cifar10ir10.indices\",\n",
    "    \"tfrecords/data/cifar-10-data-im-0.05/train.tfrecords\": \"cifar10ir20.indices\",\n",
    "    \"tfrecords/data/cifar-10-data-im-0.02/train.tfrecords\": \"cifar10ir50.indices\",\n",
    "    \"tfrecords/data/cifar-10-data-im-0.01/train.tfrecords\": \"cifar10ir100.indices\",\n",
    "    \"tfrecords/data/cifar-10-data-im-0.005/train.tfrecords\": \"cifar10ir200.indices\",\n",
    "    \"tfrecords/data/cifar-100-data-im-0.1/train.tfrecords\": \"cifar100ir10.indices\",\n",
    "    \"tfrecords/data/cifar-100-data-im-0.05/train.tfrecords\": \"cifar100ir20.indices\",\n",
    "    \"tfrecords/data/cifar-100-data-im-0.02/train.tfrecords\": \"cifar100ir50.indices\",\n",
    "    \"tfrecords/data/cifar-100-data-im-0.01/train.tfrecords\": \"cifar100ir100.indices\",\n",
    "    \"tfrecords/data/cifar-100-data-im-0.005/train.tfrecords\": \"cifar100ir200.indices\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6459668f-0aec-49ae-9f9c-b102892c65db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 14:10:30.197526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 14:10:30.676687: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 14:10:30.676730: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 14:10:30.676734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-04 14:10:31.074957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:31.092514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:31.093043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:31.093787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 14:10:31.094070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:31.094606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:31.095108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:32.012972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:32.013474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:32.013929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-04 14:10:32.014390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21605 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "for tfrecords_filepath, indices_filepath in tfrecords_to_indices.items():\n",
    "    # Load TF dataset\n",
    "    images, labels = [], []\n",
    "    for raw_record in tf.data.TFRecordDataset([tfrecords_filepath]):\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        image_bytes = example.features.feature['image'].bytes_list.value\n",
    "        image_tf = tf.io.decode_raw(image_bytes, tf.uint8)\n",
    "        image = tf.reshape(image_tf, [1, 3, 32, 32]).numpy()\n",
    "        images.extend(image)\n",
    "        label = example.features.feature['label'].int64_list.value\n",
    "        labels.extend(label)\n",
    "    cui_images = np.array(images)\n",
    "    cui_labels = np.array(labels)\n",
    "    cui_images_bytes = np.array([cui_image.tobytes() for cui_image in cui_images])\n",
    "\n",
    "    # Find indices\n",
    "    if \"cifar-100-\" in tfrecords_filepath:\n",
    "        intersection = np.in1d(cifar100_images_bytes, cui_images_bytes)\n",
    "    else:\n",
    "        intersection = np.in1d(cifar10_images_bytes, cui_images_bytes)\n",
    "    indices = np.where(intersection)[0]\n",
    "\n",
    "    # Save indices\n",
    "    with open(indices_filepath, \"w\") as f:\n",
    "        for i in indices:\n",
    "            f.write(f\"{i}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
